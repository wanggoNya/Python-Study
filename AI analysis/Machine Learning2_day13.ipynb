{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SOOYEO~1\\AppData\\Local\\Temp/ipykernel_7260/1421267803.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mhvc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVotingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'KNN'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mknn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SVM'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msvc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DT'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hard'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'knn' is not defined"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "# Machine Learning 2\n",
    "####################################################################\n",
    "\n",
    "# 앙상블 모델 ensemble model - voting\n",
    "# 여러 모델을 결합하여 성능 높이는 방법이다. \n",
    "# voting 은 여러개의 모델이 예측한 값 중에서 다수결로 최종 최종 결과를 \n",
    "# 정한다. 단, 앙상블 학습을 하게되면 개별 모델에 비해 학습 시간이\n",
    "# 오래 걸리는 단점이 있다.\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN 불러오기\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "hvc = VotingClassifier(estimators = [('KNN',knn),('SVM',svc),('DT',dtc)], voting='hard')\n",
    "\n",
    "# 학습\n",
    "hvc.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_hvc_pred = hvc.predict(X_test)\n",
    "\n",
    "# 평가\n",
    "hvc_acc = accuracy_score(y_test, y_hvc_pred)\n",
    "hvc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블 모델 ensemble model - bagging\n",
    "\n",
    "# Decision Tree 는 한개의 트리를 사용한다. 여러개의 트리를 사용하여 각 모델의 개별\n",
    "# 예측 값을 보팅을 통해 결정한다. 이처럼 같은 알고리즘 모델을 여러번 돌려 예측하는 방법을\n",
    "# bagging이라고 한다.  \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators= 100, max_depth=10, random_state=20)\n",
    "\n",
    "#학습\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "# 평가\n",
    "rfc_acc = accuracy_score(y_test, y_rfc_pred)\n",
    "rfc_acc\n",
    "\n",
    "# n_estimators= 50   0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블 모델 ensemble model - boosting\n",
    "# 부스팅은 여러개의 약한 학습기(가벼운 모델)를 순차적으로 학습한다. \n",
    "# 잘못 예측한 데이터에 대한 예측 오차를 줄이는 방향으로 모델을 계속 업데이트 한다. \n",
    "# 여러 모델을 동시에 학습하지 않고 순서대로 학습하는 점에서 배깅과는 다르다. \n",
    "\n",
    "# 캐글, 데이콘 등 경진대회에서 가장 많이 사용되는 알고리즘이다. \n",
    "# 모델 학습 속도 빠르고 예측력이 상당히 좋은 편으로 알려져 있다. \n",
    "# !pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbc = XGBClassifier(n_estimators=50, max_depth=3, random_state=20)\n",
    "\n",
    "#학습\n",
    "xgbc.fit(X_train, y_train) \n",
    "\n",
    "#예측\n",
    "y_xgbc_pred = xgbc.predict(X_test)\n",
    "\n",
    "#평가\n",
    "xgbc_acc = accuracy_score(y_test, y_xgbc_pred)\n",
    "xgbc_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과대 적합 overfitting & 과소 적합 underfitting\n",
    "\n",
    "# 과소 적합 underfitting - 머신 러닝을 진행할때 학습 시킬 데이터가 \n",
    "#                          현저히 적을때 예측력이 떨어지는 증상\n",
    "\n",
    "\n",
    "# 과대 적합 overfitting - 학습 시킨 데이터와 비슷한 것은 예측력이 우수하게 판단하지만\n",
    "#                         새로운데이터에는 예측력이 떨어지는 증상\n",
    "\n",
    "\n",
    "# solution : 데이터의 엄청나게 양을 늘린다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold 교차검증\n",
    "\n",
    "# 데이터 셋을 5개의 Fold로 분할\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=20)\n",
    "\n",
    "# 훈련용 데이터와 검증용 데이터의 행 인덱스를 각각의  Fold별로 구분하여 확인해보자\n",
    "\n",
    "num_fold = 1\n",
    "\n",
    "for tr_idx, val_idx in kfold.split(X_train):\n",
    "  print('%s Fold ---------------------------------------' % num_fold)\n",
    "  print('훈련 :', len(tr_idx), tr_idx[:])\n",
    "  print('검증 :', len(val_idx), val_idx[:])\n",
    "  num_fold = num_fold + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 데이터와 검증용 데이터의 행 인덱스를 각 fold별로 구분하여 생성\n",
    "\n",
    "val_scores = [] # 5번의 예측 정확도를 담을 리스트\n",
    "\n",
    "num_fold = 1\n",
    "\n",
    "for tr_idx, val_idx in kfold.split(X_train, y_train):\n",
    "  # 훈련용 데이터와 검증용 데이터를 행 인덱스 기준으로 추출\n",
    "  X_tr, X_val = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]   #  X 문제지\n",
    "  y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]         #  y 답안지\n",
    "\n",
    "  # 학습\n",
    "  rfc = RandomForestClassifier(max_depth= 5, random_state=20)\n",
    "  rfc.fit(X_tr,y_tr)\n",
    "\n",
    "  # 예측\n",
    "  y_val_pred = rfc.predict(X_val)\n",
    "\n",
    "  # 검증\n",
    "  val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "  # 결과 출력\n",
    "  print('%d Fold Accuracy : %.4f' % (num_fold, val_acc)) # 각각의 폴드마다의 정확도\n",
    "  val_scores.append(val_acc)\n",
    "  num_fold += 1\n",
    "\n",
    "# 평균 정확도 계산\n",
    "\n",
    "mean_score = np.mean(val_scores)\n",
    "print()\n",
    "print('평균 정확도 : ',np.round(mean_score, 2) * 100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "513dc2e41d739bb2c947903f3c0bbf636d03aa53ab50e61c694a27481c81805e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
